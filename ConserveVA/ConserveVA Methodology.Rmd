---
title: "ConserveVA Methodology"
author: "Emma Jones"
date: "4/28/2020"
output:
  html_document: 
     code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(readxl)
library(leaflet)
library(leafpop)
library(mapview)
library(inlmisc)
```

# Proposal to Integrate Ecologically Healthy Streams into Conserve Virginia
In order to conserve and improve the least disturbed watershed as indicated by macroinvertebrate communities, an option to integrate Virginia Department of Environmental Quality (VDEQ) biomonitoring results into ConserveVirginia website is for VDEQ staff to create and maintain GIS layer.

## VDEQ proposes to use the following methodology:

* Identify those watersheds containing sites indicative of good ecological conditions based on macroinvertebrate communities (VCPMI >56 or VSCI >72) using recent data collections from across the Commonwealth. 

    + Bring in VSCI, VCPMI63+Chowan, and VCPMI65-Chowan metric scores. These were queried from EDAS by VSCI > 60 or VCPMI65-Chowan/VCPMI63+Chowan > 42.

```{r bring in data}
bugScores <- read_excel('data/Drew_All.xlsx', sheet = 'VSCI') %>%
  mutate(Method = "VSCI", SCI = `Fam SCI`) %>%
  dplyr::select(BenSampID, StationID, CollDate, Lat, Long, RepNum, Order, Method, SCI) %>%  
  bind_rows(read_excel('data/Drew_All.xlsx', sheet = 'VCPMI65-Chowan') %>%
              mutate(Method = "VCPMI65-Chowan", SCI = `CPMI65-CHOWAN`, Order = NA) %>%
              dplyr::select(BenSampID, StationID, CollDate, Lat, Long, RepNum, Order, Method, SCI)) %>% 
  bind_rows(read_excel('data/Drew_All.xlsx', sheet = 'VCPMI63+Chowan') %>% 
              mutate(Method = "VCPMI63+Chowan", SCI = `CPMI63+CHOWAN`, Order = NA) %>%
              dplyr::select(BenSampID, StationID, CollDate, Lat, Long, RepNum, Order, Method, SCI))
```
    
    + Average SCI scores by same day replicate samples, then average by stations, then filter VCPMI >56 or VSCI >72.
    
```{r filter by thresholds}
bugFilter <- bugScores %>%
  group_by(StationID, CollDate) %>%
  mutate(sameDaySCIAverage = mean(SCI)) %>%
  ungroup() %>%
  group_by(StationID) %>%
  mutate(stationSCIAverage = mean(sameDaySCIAverage)) %>%
  filter(CollDate >= as.Date('2013-01-01')) %>%
  filter(Method == 'VSCI' && stationSCIAverage >= 72 |
           Method %in% c('VCPMI65-Chowan', 'VCPMI63+Chowan') && stationSCIAverage >= 56 ) %>% 
  distinct(StationID, .keep_all = T) %>%
  dplyr::select(StationID, Lat, Long, Order, Method, stationSCIAverage)
```
    
* Intersect 12 digit HUCs with biological station location to identify the HUCs that may qualify for inclusion.

    + Bring in HUC12 information (VA assessment region layer).

```{r read in HUC12 layer}
HUC12 <- st_read("GIS/AssessmentRegions_VA84_basins.shp")
```

    + Spatially join the bug stations that passed the reference filters with the HUC12 basin information. First drop sites missing location information as no spatial work can be done without that piece of data.
    
```{r join HUC12}
bugFilterHUC12 <- bugFilter %>%
  filter(!is.na(Lat) | !is.na(Long)) %>% # drop any sites missing location
  mutate(Long = ifelse(Long > 0, -(Long), Long)) %>% # make sure all sites in western hemisphere
  st_as_sf(coords = c("Long", "Lat"),  # make spatial layer using these columns
           remove = F, # dont remove these lat/lon cols from df, will need later
           crs = 4326) %>% # add coordinate reference system, WGS84
  st_intersection(HUC12)
```

Quick QA detour to make sure all spatial info correctly joined.

```{r QA sites}
missingSites <- bugFilter$StationID[!(bugFilter$StationID %in% bugFilterHUC12$StationID)]

missingSites_sf <- filter(bugScores, StationID %in% missingSites) %>% 
       distinct(StationID, .keep_all = T) %>%
  filter(!is.na(Lat) | !is.na(Long)) %>% # drop any sites missing location
  st_as_sf(coords = c("Long", "Lat"),  # make spatial layer using these columns
             remove = T, # dont remove these lat/lon cols from df, will need later
             crs = 4326)

# plot missing sites vs HUC12 layer
mapview(missingSites_sf) + mapview(HUC12) 


# Fix site outside VA (just outside of HUC12 extent)
x4ACAN <- filter(bugFilter, StationID == '4ACAN000.80') %>% # site outside VA
   st_as_sf(coords = c("Long", "Lat"),  # make spatial layer using these columns
           remove = F, # dont remove these lat/lon cols from df, will need later
           crs = 4326) %>% # add coordinate reference system, WGS84
  bind_cols(filter(HUC12, HUC12 == '030101040109') %>% # appropriate HUC12 
              st_drop_geometry())


# Fix site in WV (incorrect lat/lng)
x4ADAN <- filter(bugFilter, StationID == '4ADAN199.71') %>% # site in WV
  mutate(Lat = 36.7024)


bugFilterHUC12 <- bugFilter %>%
  filter(!StationID == '4ADAN199.71') %>% # site in WV
  bind_rows(x4ADAN) %>% # fixed lat/lngs
  filter(!is.na(Lat) | !is.na(Long)) %>% # drop any sites missing location
  mutate(Long = ifelse(Long > 0, -(Long), Long)) %>% # make sure all sites in western hemisphere
  st_as_sf(coords = c("Long", "Lat"),  # make spatial layer using these columns
           remove = F, # dont remove these lat/lon cols from df, will need later
           crs = 4326) %>% # add coordinate reference system, WGS84
  st_intersection(HUC12)

# Add manual fixes back to whole dataset
bugFilterHUC12 <- bugFilterHUC12 %>%
  rbind(  x4ACAN  ) # rbind works best for sf objects over bind_rows()

rm(x4ACAN); rm(x4ADAN)

```
  
    Make sure all sites match CEDS info. Using Roger's conventionals pull to double check
    
```{r conventionals check, eval = FALSE}
# Bring in 2020 IR data pull (2013-2018 data), will still need to bring in 2017&2018 sites
# Goal: filter out all BRRO sites to get a list of which sites were sampled each year
#  and frequency if possible
conventionals <- read_csv('C:/HardDriveBackup/R/GitHub/Rivers-StreamsAssessment/R&S_app_v4/data/final2020data/CEDSWQM_2020_IR_DATA-CONVENTIONALS_20190305.csv') 
conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")

#summary(conventionals$FDT_DATE_TIME2)

# Now add recent data (2018-Nov 2019- the day Roger made the data pull)
# already limited to BRRO (SCRO and WCRO)
conventionals2 <- read_excel('C:/HardDriveBackup/R/GitHub/AmbientNetworkPlanning/for2020/data/CEDSWQM_CONVENTIONALS_2018+.xlsx')
conventionals2$FDT_DATE_TIME2 <- as.POSIXct(conventionals2$FDT_DATE_TIME, format="%m/%d/%Y %H:%M")
conventionals2$FDT_DATE_TIME <- as.character(conventionals2$FDT_DATE_TIME) # for smashing with original conventionals
summary(conventionals2$FDT_DATE_TIME2)

# filter data to just 2019 to not duplicate data from 2018
conventionals2019 <- filter(conventionals2, FDT_DATE_TIME2 > '2018-12-31 23:59:00')
summary(conventionals2019$FDT_DATE_TIME2)
# cool.
#glimpse(conventionals2019)

# what is in conventionals that isn't in conventionals2019??
names(conventionals)[!names(conventionals) %in% names(conventionals2019)]

conventionalsAll <- bind_rows(conventionals,conventionals2019) %>%
  # get groundwater sites out of here
  filter(FDT_SPG_CODE != 'GW')

conventionals <- filter(conventionalsAll, !is.na(Latitude)|!is.na(Longitude)) # remove sites without coordinates

conventionals_sf <- distinct(conventionals, FDT_STA_ID, .keep_all = T) %>%
  select(FDT_STA_ID:FDT_SPG_CODE, Latitude:STA_CBP_NAME) %>% # drop data to avoid any confusion
  mutate(StationID = FDT_STA_ID) %>% # make a joining column for later
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                        remove = F, # don't remove these lat/lon cols from df
                        crs = 4326)

rm(conventionalsAll);rm(conventionals2);rm(conventionals2019); rm(conventionals) # remove conventionals datasets to save memory


# the good stuff
conventionalsCheck <- left_join(bugFilter, conventionals_sf, by = 'StationID') %>%
  mutate( Lat = round(Lat, digits = 2), Long = round(Long, digits = 2),
          Latitude = round(Latitude, digits = 2), Longitude = round(Longitude, digits = 2),
    latDiff = ifelse(Lat != Latitude, 'Flag', NA),
         lngDiff = ifelse(Long != Longitude, 'Flag', NA)) %>%
  dplyr::select(latDiff, lngDiff, Latitude, Longitude, everything()) %>%
  filter(latDiff == 'Flag' | lngDiff == 'Flag')

write.csv(conventionalsCheck, 'conventionalsCheck.csv')
```

* The qualifying HUCs are delineated to pour point (biological station location) using StreamSTATS methodology (https://streamstats.usgs.gov/) to create watershed layer (based on the default USGS StreamStats 1:24,000 NHD layer)

    + First, bring in the functions necessary to scrape streamstats API.

```{r bring in autodelineation functions}
source('methods/StreamStatsAutoDelineation.R')
```
    
    + Then summarize delineation requirements by basin to strategize splitting up workload efficiently.
    
```{r summarize sites by major basin}
bugFilterHUC12 %>%
  group_by(Basin) %>%
  summarize(total = n())
```

    + And start delineating by Basin.
    
```{r Delineate by specified basin}
# unique basin names to choose from
unique(bugFilterHUC12$Basin)

# Choose your basin
basinName <- 'Potomac River Basin'

basin <- filter(bugFilterHUC12, Basin == basinName)

basinData <- streamStats_Delineation(state= 'VA',
                                     longitude = basin$Long, 
                                     latitude = basin$Lat, 
                                     UID = basin$StationID)
```

    + Then we need to organize these files and check they delineated properly.
    
```{r organize spatial results}
# organize into appropriate files, lossy move here- automatically removes sites with no data
watersheds <- basinData$polygon %>%
  reduce(rbind)
points <- basinData$point %>%
  reduce(rbind) 
```
    
    + Run back out to streamstats and get anyone that is missing
    
```{r fix missing sites (rerun as necessary)}
# fix anything that is missing
if(nrow(points) != nrow(watersheds) | nrow(basin) != nrow(watersheds)){
  missing <- unique(
    c(points$UID[!(points$UID %in% watersheds$UID)],
      eachYear$StationID[!(eachYear$StationID %in% watersheds$UID)]))
  missingDat <- filter(basin, StationID %in% missing)
  
  dat <- streamStats_Delineation(state= 'VA', 
                                 longitude = missingDat$LongitudeDD, 
                                 latitude = missingDat$LatitudeDD, 
                                 UID = missingDat$StationID)
  
  watersheds_missing <- dat$polygon %>%
    reduce(rbind)
  
  points_missing <- dat$point %>%
    reduce(rbind)
  
  watersheds <- rbind(watersheds, watersheds_missing) %>%
    arrange(UID)
  
  points <- rbind(points, points_missing) %>%
    arrange(UID)
  
  rm(missingDat); rm(dat); rm(watersheds_missing)
}

```
    
    + We are going to do manual QA for each watershed. Bring in Probabilistic Monitoring sample frame for stream order information.

```{r visual QA mapping function}
vafrm <- st_read('GIS/vafrm_99_05Albers.shp') %>%
  st_transform(4326) # change to WGS84 for web mapping

mapMe <- function(watershedChoice, pointChoice, initialData, probFrame){
  pointChoice <- mutate(pointChoice, StationID = UID) %>%
    left_join(initialData, by = 'StationID')
  
  segments <- suppressWarnings(st_intersection(probFrame, st_buffer(watershedChoice,0))) # buffer by 0 helps with topology issues
  
  CreateWebMap(maps = c("Topo","Imagery","Hydrography"), collapsed = TRUE) %>%
    #setView(-78, 37.5, zoom=6) %>%
    addPolygons(data= watershedChoice,  color = 'black', weight = 1,
                  fillColor='blue', fillOpacity = 0.5,stroke=0.1,
                  group="Watershed",
                  popup=leafpop::popupTable(watershedChoice, zcol=c('UID'))) %>%
    addPolylines(data = segments,  color = 'blue', weight =3,
                 group="Strahler Info", label = ~STRAHLER,
                 popup=leafpop::popupTable(segments, zcol=c('STRAHLER'))) %>% hideGroup("Strahler Info") %>%
    addCircleMarkers(data = pointChoice, color='orange', fillColor='black', radius = 5,
                     fillOpacity = 0.5,opacity=0.5,weight = 1,stroke=T, group="Station",
                     label = ~UID,
                     popup=leafpop::popupTable(pointChoice)) %>%
    addLayersControl(baseGroups=c("Topo","Imagery","Hydrography"),
                       overlayGroups = c('Station',"Strahler Info", 'Watershed'),
                       options=layersControlOptions(collapsed=T),
                       position='topleft')
}
```

    + And double check all watersheds are appropriately delineated.

```{r visual QA}
# Can be reviewed one at a time
mapMe(watersheds[1,], points[1,], bugFilter, vafrm)

# or all at once
mapMe(watersheds, points, bugFilter, vafrm)

```

    + Anything look off? If so, filter out those StationID's before they get saved to a shapefile. You will have to go to streamstats and manually choose the appropriate stream segment, then merge back to full basin watersheds shapefile.
    
```{r drop these sites for manual autodelineation}
# identify problem sites
dropMe <- c('1AXJI000.38')

# drop the sites identified
watersheds <- filter(watersheds, ! UID %in% dropMe)
# note, we are not dropping the DEQ lat/lng from the points file because this is the most accurate location information

# organize lat/lngs for problem sites for delineation on streamstats website
dropMe <- tibble(StationID = dropMe) %>%
  left_join(basin, by = 'StationID')
dropMe
```
    
For the station(s) listed above (if any):
1. Go to https://streamstats.usgs.gov/ss/
2. Enter each lat/lng, separated by a comma
3. Choose the appropriate state (VA)
4. Delineate: Use the 1:100k strahler stream order from the Order field above to choose the appropriate streamstats (1:24k) stream segment.
5. Download Basin as a shapefile.
6. Extract the zip file.
7. Rename the 'globalwatershed' file to the name of the StationID
**Pro Tip- select all four files (.dbf, .prj, .shp, .shx), press F2, paste the StationID into one file, press Enter... all renamed **
8. Move the renamed watershed to GIS/delineations/streamStatsFixes/basinName (make sure there is a folder with the appropriate basin name)
![directorystructure](methods/directoryStructure.png)

    + Now bring back in the manually autodelineated watersheds and combine with the watersheds that passed QA filters.
    
```{r bring in watersheds}
# Where did you save files relative to the working directory?
savedHere <- 'GIS/delineations/streamStatsFixes/Potomac'

# read in new watersheds after manual QA
shapes <- gsub('.prj','',list.files( savedHere, pattern="*.prj", full.names=F))
filenames <- paste0(savedHere, '/',shapes, '.shp')
  
# read in shapefiles and organize
newSheds <- filenames %>%
  map(st_read) %>%
  map2(shapes,~mutate(.x,UID=.y)) %>% # make a StationID column
  map(~dplyr::select(.,UID)) %>%
  reduce(rbind) 

# Combine with QAed watersheds
watersheds <- rbind(watersheds, newSheds) %>%
  arrange(UID)
  
```
    
    + Double check everything looks good in the basin.
    
```{r map double check}
mapMe(watersheds, points, bugFilter, vafrm)
```
    + Save QAed watersheds and sites as shapefiles.
    
```{r save shapefiles}
st_write(watersheds, paste0('GIS/delineations/', basinName,'_StreamStats_watersheds.shp'))
st_write(points, paste0('GIS/delineations/', basinName,'_StreamStats_points.shp'))
```
    
    