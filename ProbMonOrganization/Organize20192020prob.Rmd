---
title: "Organize 2019-2020 Probmon Sites"
author: "Emma Jones"
date: "10/28/2021"
output: html_document
---

## Background

This script walks users through the process of identifying which stations sampled in 2019-2020 need watersheds delineated, delineates the needed sites, and how to run subsequent landcover analyses for 2022IR report.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(sf)

source('./organizeShapefiles.R') # need to run in console, not from chunk
```

Below is the final 2019-2020 dataset filtered by what was sampled (wadeable and boatable combined for delineation purposes).

```{r}
stations <- bind_rows(
  readxl::read_excel('C:/HardDriveBackup/R/GitHub/ProbMon-Integrated-Reports/2022ProbChapter/originalData/Wadeable_ProbMon_2019-2020_Final.xlsx',
                     sheet = 'All20192020StationInfo') %>% 
    filter(status %in% c('TS')),
  readxl::read_excel('C:/HardDriveBackup/R/GitHub/ProbMon-Integrated-Reports/2022ProbChapter/originalData/Wadeable_ProbMon_2019-2020_Final.xlsx',
                       sheet = 'All20192020StationInfo') %>% 
    filter(status %in% c('NT') & str_detect(comment, '(sampled)')) %>% 
    filter(!str_detect(comment, 'not sampled'))) %>% 
  dplyr::select(StationID, Year, StationID_Trend, Longitude = LongitudeDD, Latitude = LatitudeDD)

```


Which stations actually need watershed info?

```{r spatial info already available}
watersheds <- #st_read('D:/evjones/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb', 'AllWatersheds_through2016') %>% 
  st_read('C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb', 'AllWatersheds_through2020') %>% 
  filter(StationID %in% stations$StationID)

# so what isn't in there?
missingWatersheds <- filter(stations, ! StationID %in% watersheds$StationID) 

```




Bring in autodelineation functions

```{r bring in autodelineation functions}
source('./ConserveVA/methods/StreamStatsAutoDelineation.R')
```


### Delineate watersheds

```{r delineate watersheds}
streamStatsResults <- streamStats_Delineation(state= 'VA',
                                      longitude = missingWatersheds$Longitude, 
                                      latitude = missingWatersheds$Latitude, 
                                      UID = missingWatersheds$StationID)
```

    + Then we need to organize these files and check they delineated properly.
    
```{r organize spatial results}
# organize into appropriate files, lossy move here- automatically removes sites with no data
watersheds <- streamStatsResults$polygon %>%
  reduce(rbind) %>%
  arrange(UID)
points <- streamStatsResults$point %>%
  reduce(rbind) %>%
  arrange(UID)
```
    
    + Run back out to streamstats and get anyone that is missing
    
```{r fix missing sites (rerun as necessary)}
# fix anything that is missing
if(nrow(points) != nrow(watersheds) | nrow(missingWatersheds) != nrow(watersheds)){
  missing <- unique(
    c(as.character(points$UID[!(points$UID %in% watersheds$UID)]),
      as.character(missingWatersheds$StationID[!(missingWatersheds$StationID %in% watersheds$UID)])))
  missingDat <- filter(missingWatersheds, StationID %in% missing)
  
  #remove missing site from the paired dataset
  points <- filter(points, ! UID %in% missing)
  watersheds <- filter(watersheds, ! UID %in% missing)
  
  dat <- streamStats_Delineation(state= 'VA', 
                                 longitude = missingDat$Longitude, 
                                 latitude = missingDat$Latitude, 
                                 UID = missingDat$StationID)
  
  watersheds_missing <- dat$polygon %>%
    reduce(rbind)
  
  points_missing <- dat$point %>%
    reduce(rbind)
  
  watersheds <- rbind(watersheds, watersheds_missing) %>%
    arrange(UID)
  
  points <- rbind(points, points_missing) %>%
    arrange(UID)
  
  rm(missingDat); rm(dat); rm(watersheds_missing); rm(points_missing)
}

```
   
   
   
QA can be done in R, but with so many it is faster to do in ArcGIS right now.

Bring in somewhat consolidated (Fall 2019 only) bio reports that link StationID to stream order for QA.

```{r stream order}
orderInfo <- bind_rows(
  readxl::read_excel('C:/HardDriveBackup/R/GitHub/ProbMon-Integrated-Reports/2022ProbChapter/originalData/Wadeable_ProbMon_2019-2020_Final.xlsx',
                     sheet = 'All20192020StationInfo') %>% 
    filter(status %in% c('TS')),
  readxl::read_excel('C:/HardDriveBackup/R/GitHub/ProbMon-Integrated-Reports/2022ProbChapter/originalData/Wadeable_ProbMon_2019-2020_Final.xlsx',
                       sheet = 'All20192020StationInfo') %>% 
    filter(status %in% c('NT') & str_detect(comment, '(sampled)')) %>% 
    filter(!str_detect(comment, 'not sampled'))) %>% 
  dplyr::select(StationID, weightcategory)

watersheds <- left_join(watersheds, orderInfo, by = c('UID' = 'StationID')) %>%
  dplyr::select(UID,  weightcategory, everything()) %>% 
  filter(!is.na(UID))
```


```{r}
st_write(points, 'C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/YearlyAnalyses/20192020_StreamStats/rawPointsFinalList.shp')
st_write(watersheds, 'C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/YearlyAnalyses/20192020_StreamStats/rawWatershedsFinalList.shp')
```




### Run landcover assessment

First bring in QAed watersheds and identify which watersheds require landcover data. Make sure no one is missing.

```{r QAed watersheds}
QAwatersheds <- st_read('C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb', 'AllWatersheds_through2020') 
stations$StationID[! stations$StationID %in% QAwatersheds$StationID]
  filter(QAwatersheds,StationID %in% missingWatersheds$StationID)
```
Now run landcover analysis in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\landcoverAnalysis_20192020.R

```{r landcover analysis}
#source('C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\landcoverAnalysis_20192020.R')
```























# first attempt with phab database



Right now there is no official 2019-2020 sampled dataset so working with the what is in the Phab database as a starting point.

```{r phab sites new}
phab <- readxl::read_excel('C:/HardDriveBackup/ProbMon/2019/Reach.xlsx') %>% 
  filter(Date > as.Date("2018-12-31")) %>% 
  
  filter(StationID %in% c('1AGO039.69' , '4ASRE069.63', '2-POT0.38.71', '9CVR004.86', '8XIW000.55',
                          '9LRR012.30', '9XBB000.86'))

  
  mutate(`StationID` = case_when(StationID == '1AGO039.69' ~ '1AGOO039.63',
                                 StationID == '4ASRE069.63' ~ '4ASRE063.69',
                                 StationID == '2-POT0.38.71' ~ '2-POT038.71',
                                 StationID == '9CVR004.86' ~ '9-CVR004.86',
                                 StationID == '8XIW000.55' ~ '8-XIW000.55',
                                 StationID == '9LRR012.30' ~ '9-LRR012.30',
                                 StationID == '9XBB000.86' ~ '9-XBB000.86',
                                 TRUE ~ as.character(StationID) )) 
```

### Connect to pinned data to get site lat/lngs from CEDS.

```{r pinned data}
library(pool)
library(pins)
library(config)

# get configuration settings
conn <- config::get("connectionSettings")

# use API key to register board
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                          server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

stations <- pin_get('ejones/WQM-Sta-GIS-View-Stations',  board = "rsconnect") %>% 
  filter(Station_Id %in% phab$StationID)

# Quick QA, which stations are in phab database but not in CEDS?
stationIssues <- phab$StationID[! phab$StationID %in% stations$Station_Id]

filter(phab, StationID %in% stationIssues) %>% 
  mutate(`Real StationID` = case_when(StationID == '1AGO039.69' ~ '1AGOO039.63',
                                      StationID == '4ASRE069.63' ~ '4ASRE063.69',
                                      StationID == '2-POT0.38.71' ~ '2-POT038.71',
                                      StationID == '9CVR004.86' ~ '9-CVR004.86',
                                      StationID == '8XIW000.55' ~ '8-XIW000.55',
                                      StationID == '9LRR012.30' ~ '9-LRR012.30',
                                      StationID == '9XBB000.86' ~ '9-XBB000.86',
                                      TRUE ~ NA_character_ ))
```

Which stations actually need watershed info?

```{r spatial info already available}
watersheds <- #st_read('D:/evjones/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb', 'AllWatersheds_through2016') %>% 
  st_read('C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb', 'AllWatersheds_through2018') %>% 
  filter(StationID %in% stations$Station_Id)

# so what isn't in there?
missingWatersheds <- filter(stations, ! Station_Id %in% watersheds$StationID) 

```

Bring in somewhat consolidated (Fall 2019 only) bio reports that link StationID to stream order for QA.

```{r stream order}
orderInfo <- readxl::read_excel('C:/HardDriveBackup/ProbMon/2019/RegionalResults_2019_EVJconsolidation.xlsx',sheet = 'EVJ Consolidation')

watersheds <- left_join(watersheds, dplyr::select(orderInfo, REGION:MDCATY), by = c('UID' = 'DEQSITEID')) %>%
  dplyr::select(UID, REGION, MDCATY, everything()) %>% 
  filter(!is.na(UID))
```


```{r}
st_write(points, 'C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/YearlyAnalyses/20192020_StreamStats/rawPoints2.shp')
st_write(watersheds, 'C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/YearlyAnalyses/20192020_StreamStats/rawWatersheds11082021.shp')
```